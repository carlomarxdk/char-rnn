{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from simple.model import *\n",
    "from simple.dataloader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextDataset(sequence_length=100)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = CharRNN(input_size=loader.sequence_length, \n",
    "                output_size=len(loader.chars), \n",
    "                hidden_size=12, num_layers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "epoch = range(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 4.379568\n",
      "[2, 2, 2, 2, 2, 34, 2, 2, 34, 34, 2, 58, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[49, 45, 38, 47, 37, 42, 37, 2, 51, 54, 52, 52, 42, 34, 47, 2, 49, 38, 48, 49, 45, 38, 12, 2, 52, 48, 2, 52, 42, 46]\n",
      "Mean Loss: 4.3791056\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 34, 2, 2, 2, 2, 79, 2, 2, 34, 2, 2, 2, 2]\n",
      "[48, 37, 72, 35, 58, 12, 2, 33, 49, 34, 47, 42, 38, 33, 3, 78, 1, 0, 1, 0, 46, 42, 53, 58, 34, 2, 34, 56, 34, 42]\n",
      "Mean Loss: 4.3785143\n",
      "[2, 58, 2, 34, 2, 2, 2, 2, 2, 2, 2, 2, 34, 52, 52, 2, 2, 2, 2, 34, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[35, 54, 53, 2, 53, 41, 34, 53, 1, 0, 41, 38, 2, 36, 48, 54, 45, 37, 2, 47, 48, 53, 2, 52, 38, 38, 13, 1, 0, 1]\n",
      "Mean Loss: 4.3777823\n",
      "[2, 2, 2, 2, 52, 34, 34, 2, 2, 2, 2, 2, 2, 2, 2, 34, 34, 2, 34, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[48, 2, 48, 54, 51, 2, 45, 42, 53, 53, 45, 38, 2, 53, 48, 56, 47, 2, 53, 48, 2, 52, 38, 53, 53, 45, 38, 2, 54, 49]\n",
      "Mean Loss: 4.3768597\n",
      "[2, 2, 52, 2, 2, 2, 34, 34, 34, 2, 34, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 34, 2]\n",
      "[38, 1, 0, 49, 34, 52, 52, 34, 40, 38, 13, 2, 53, 41, 38, 58, 2, 56, 38, 47, 53, 2, 42, 47, 53, 48, 2, 53, 41, 34]\n",
      "Mean Loss: 4.374925\n",
      "[2, 2, 2, 2, 2, 2, 34, 2, 2, 34, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 34]\n",
      "[37, 48, 47, 76, 53, 2, 36, 34, 45, 45, 1, 0, 41, 42, 46, 2, 52, 48, 12, 2, 41, 38, 2, 56, 42, 45, 45, 2, 35, 38]\n",
      "Mean Loss: 4.3723054\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 34, 34, 48, 2, 34, 2, 2, 2, 2, 2, 48, 48, 2, 2, 48, 48, 2, 2, 2, 2, 2]\n",
      "[2, 46, 42, 53, 58, 34, 2, 34, 36, 36, 54, 52, 38, 37, 2, 46, 38, 2, 48, 39, 1, 0, 42, 53, 12, 2, 52, 48, 2, 47]\n",
      "Mean Loss: 4.368963\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 48, 48, 2, 2, 48, 2, 2, 2, 2, 2, 2, 48, 48, 34, 48, 52, 52]\n",
      "[2, 52, 54, 37, 37, 38, 47, 45, 58, 2, 56, 42, 53, 41, 2, 40, 38, 47, 54, 42, 47, 38, 2, 52, 54, 51, 49, 51, 42, 52]\n",
      "Mean Loss: 4.364722\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 34, 48, 48, 34, 2, 48, 48, 48, 48]\n",
      "[2, 35, 38, 40, 34, 47, 2, 35, 48, 56, 42, 47, 40, 2, 42, 47, 2, 34, 45, 45, 2, 37, 42, 51, 38, 36, 53, 42, 48, 47]\n",
      "Mean Loss: 4.3598247\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 48, 48, 48, 48, 48, 48, 48, 2, 2, 2, 2, 2]\n",
      "[2, 54, 49, 2, 34, 40, 34, 42, 47, 13, 1, 0, 1, 0, 42, 55, 34, 47, 2, 43, 54, 46, 49, 38, 37, 2, 54, 49, 2, 34]\n",
      "Mean Loss: 4.354387\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 48, 48, 48, 48, 2, 2, 2, 48, 2, 48, 48, 48, 48, 48, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[41, 38, 2, 40, 48, 48, 52, 38, 76, 52, 2, 47, 38, 36, 44, 1, 0, 56, 34, 52, 2, 35, 51, 48, 44, 38, 47, 2, 42, 47]\n",
      "Mean Loss: 4.348925\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 48, 48, 48, 48, 48, 2, 48, 48, 48, 48, 48, 48]\n",
      "[2, 39, 51, 48, 46, 1, 0, 37, 48, 42, 47, 40, 2, 52, 48, 13, 1, 0, 1, 0, 42, 53, 2, 56, 34, 52, 2, 34, 2, 35]\n",
      "Mean Loss: 4.3434615\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0, 36, 48, 47, 36, 45, 54, 52, 42, 48, 47, 13, 1, 0, 1, 0, 77, 37, 42, 37, 2, 41, 38, 2, 47, 38, 55, 38, 51, 2]\n",
      "Mean Loss: 4.338204\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[58, 2, 49, 34, 51, 39, 38, 47, 48, 55, 42, 53, 36, 41, 2, 52, 38, 38, 46, 38, 37, 2, 53, 48, 2, 35, 38, 2, 40, 48]\n",
      "Mean Loss: 4.3330364\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[47, 40, 52, 2, 53, 41, 34, 53, 2, 39, 58, 48, 37, 48, 51, 2, 49, 34, 55, 45, 48, 55, 42, 53, 36, 41, 2, 56, 34, 52]\n",
      "Mean Loss: 4.3281865\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[53, 41, 38, 51, 13, 78, 1, 0, 1, 0, 46, 48, 53, 41, 38, 51, 2, 52, 41, 48, 48, 44, 2, 41, 38, 51, 2, 41, 38, 34]\n",
      "Mean Loss: 4.32287\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[54, 53, 2, 42, 53, 2, 48, 47, 2, 46, 58, 2, 41, 38, 34, 37, 2, 34, 47, 37, 2, 49, 38, 51, 41, 34, 49, 52, 2, 58]\n",
      "Mean Loss: 4.3172526\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 41, 38, 51, 13, 2, 75, 36, 48, 46, 38, 12, 76, 2, 52, 34, 42, 37, 2, 41, 38, 12, 1, 0, 75, 36, 34, 53, 36, 41]\n",
      "Mean Loss: 4.311519\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 37, 38, 39, 42, 34, 47, 53, 45, 58, 2, 34, 53, 2, 53, 41, 38, 2, 36, 48, 46, 49, 34, 47, 58, 2, 34, 47, 37, 1]\n",
      "Mean Loss: 4.305614\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[42, 47, 42, 47, 40, 2, 53, 56, 48, 2, 53, 41, 48, 54, 52, 34, 47, 37, 2, 53, 41, 51, 38, 38, 2, 41, 54, 47, 37, 51]\n",
      "Mean Loss: 4.299462\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[58, 2, 39, 48, 51, 2, 42, 53, 13, 2, 34, 47, 37, 2, 34, 52, 2, 42, 45, 45, 72, 45, 54, 36, 44, 2, 56, 48, 54, 45]\n",
      "Mean Loss: 4.2937655\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[48, 54, 45, 37, 2, 47, 38, 55, 38, 51, 2, 41, 34, 55, 38, 2, 48, 49, 38, 47, 38, 37, 2, 53, 41, 38, 2, 37, 48, 48]\n",
      "Mean Loss: 4.2875576\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[56, 41, 38, 51, 38, 2, 42, 53, 2, 45, 34, 58, 2, 34, 53, 2, 53, 41, 34, 53, 2, 46, 48, 46, 38, 47, 53, 13, 2, 42]\n",
      "Mean Loss: 4.2818975\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[58, 2, 42, 52, 2, 39, 48, 54, 47, 37, 2, 42, 47, 2, 52, 48, 37, 48, 46, 13, 2, 37, 42, 37, 2, 58, 48, 54, 2, 44]\n",
      "Mean Loss: 4.2767053\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[45, 45, 58, 2, 56, 34, 52, 2, 34, 2, 46, 34, 47, 2, 48, 39, 2, 53, 34, 45, 38, 47, 53, 2, 34, 47, 37, 2, 48, 47]\n",
      "Mean Loss: 4.2716694\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[42, 37, 2, 47, 48, 53, 41, 42, 47, 40, 1, 0, 34, 35, 48, 54, 53, 2, 42, 55, 34, 47, 2, 53, 48, 2, 58, 48, 54, 2]\n",
      "Mean Loss: 4.266922\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[41, 2, 34, 47, 37, 2, 39, 45, 58, 2, 48, 39, 39, 2, 42, 47, 53, 48, 2, 53, 41, 38, 2, 54, 47, 44, 47, 48, 56, 47]\n",
      "Mean Loss: 4.2633276\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 41, 42, 46, 2, 35, 58, 2, 53, 41, 38, 2, 49, 51, 48, 52, 38, 36, 54, 53, 48, 51, 2, 42, 52, 2, 42, 47, 36, 48]\n",
      "Mean Loss: 4.260008\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[48, 53, 2, 48, 39, 2, 41, 38, 51, 52, 12, 2, 41, 34, 37, 2, 35, 38, 38, 47, 2, 45, 38, 52, 52, 1, 0, 49, 42, 36]\n",
      "Mean Loss: 4.2571387\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 58, 48, 54, 51, 1, 0, 46, 34, 46, 46, 34, 2, 53, 48, 48, 12, 2, 56, 42, 53, 41, 2, 40, 51, 38, 34, 53, 2, 45]\n",
      "Mean Loss: 4.2549186\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[54, 52, 41, 38, 47, 44, 34, 2, 45, 34, 54, 40, 41, 38, 37, 2, 42, 47, 2, 52, 49, 42, 53, 38, 2, 48, 39, 2, 41, 38]\n",
      "Mean Loss: 4.253159\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[55, 38, 2, 42, 53, 2, 53, 48, 2, 58, 48, 54, 13, 2, 45, 48, 48, 44, 2, 34, 53, 2, 42, 53, 2, 39, 51, 48, 46, 2]\n",
      "Mean Loss: 4.252112\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[34, 45, 58, 48, 52, 41, 34, 13, 2, 53, 41, 38, 1, 0, 45, 34, 53, 53, 38, 51, 2, 56, 38, 47, 53, 2, 47, 38, 34, 51]\n",
      "Mean Loss: 4.251043\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[52, 52, 2, 58, 48, 54, 2, 39, 48, 51, 2, 40, 51, 38, 34, 53, 1, 0, 52, 38, 51, 55, 42, 36, 38, 2, 42, 47, 2, 53]\n",
      "Mean Loss: 4.2503147\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[36, 2, 34, 53, 2, 52, 54, 36, 41, 2, 34, 2, 46, 48, 46, 38, 47, 53, 29, 78, 2, 52, 41, 38, 2, 34, 37, 37, 38, 37]\n",
      "Mean Loss: 4.2495565\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[76, 53, 2, 52, 34, 58, 2, 55, 38, 51, 58, 2, 41, 48, 47, 38, 52, 53, 12, 2, 35, 54, 53, 2, 13, 13, 13, 2, 42, 53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 4.2487803\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[56, 13, 1, 0, 1, 0, 1, 0, 1, 0, 36, 41, 34, 49, 53, 38, 51, 2, 55, 13, 2, 47, 48, 53, 2, 58, 48, 54, 12, 2]\n",
      "Mean Loss: 4.248069\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[46, 38, 47, 53, 42, 47, 40, 2, 34, 47, 37, 2, 38, 55, 42, 45, 1, 0, 42, 46, 49, 51, 38, 52, 52, 42, 48, 47, 2, 45]\n",
      "Mean Loss: 4.247569\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[53, 2, 52, 48, 46, 38, 53, 41, 42, 47, 40, 2, 55, 38, 51, 58, 2, 54, 47, 52, 38, 38, 46, 45, 58, 2, 56, 34, 52, 1]\n",
      "Mean Loss: 4.2467127\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[38, 51, 58, 2, 48, 49, 49, 48, 52, 42, 53, 38, 2, 36, 48, 47, 36, 45, 54, 52, 42, 48, 47, 29, 2, 53, 41, 38, 51, 38]\n",
      "Mean Loss: 4.246692\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[53, 41, 38, 51, 76, 52, 2, 41, 48, 54, 52, 38, 12, 2, 34, 47, 37, 2, 56, 41, 58, 2, 37, 38, 49, 42, 36, 53, 2, 46]\n",
      "Mean Loss: 4.246352\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[53, 41, 38, 1, 0, 49, 38, 51, 46, 42, 52, 52, 42, 48, 47, 2, 48, 39, 2, 53, 41, 38, 2, 36, 48, 49, 58, 51, 42, 40]\n",
      "Mean Loss: 4.2458653\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[52, 53, 54, 49, 42, 37, 12, 2, 34, 56, 39, 54, 45, 45, 58, 2, 52, 53, 54, 49, 42, 37, 12, 78, 2, 52, 34, 42, 37, 2]\n",
      "Mean Loss: 4.2456675\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[41, 38, 2, 34, 37, 37, 38, 37, 12, 2, 38, 57, 53, 51, 38, 46, 38, 45, 58, 2, 49, 48, 45, 42, 53, 38, 45, 58, 13, 2]\n",
      "Mean Loss: 4.24544\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[55, 38, 2, 53, 41, 38, 38, 2, 53, 41, 38, 51, 38, 12, 2, 34, 47, 37, 2, 39, 51, 48, 46, 2, 53, 41, 38, 51, 38, 1]\n",
      "Mean Loss: 4.2455544\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[58, 12, 2, 35, 34, 47, 44, 52, 12, 2, 34, 47, 37, 2, 52, 48, 2, 48, 47, 74, 52, 48, 46, 38, 53, 41, 42, 47, 40, 2]\n",
      "Mean Loss: 4.244836\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[41, 38, 2, 40, 48, 37, 2, 48, 39, 1, 0, 45, 42, 39, 38, 2, 52, 41, 48, 54, 45, 37, 2, 35, 38, 2, 34, 47, 47, 42]\n",
      "Mean Loss: 4.2451506\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[40, 38, 53, 2, 56, 38, 45, 45, 13, 78, 1, 0, 1, 0, 77, 42, 55, 34, 47, 2, 41, 34, 52, 2, 34, 2, 52, 53, 51, 48]\n",
      "Mean Loss: 4.2444353\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[38, 2, 56, 48, 51, 37, 2, 77, 36, 41, 42, 47, 38, 52, 38, 78, 1, 0, 39, 45, 34, 52, 41, 38, 37, 2, 42, 47, 53, 48]\n",
      "Mean Loss: 4.2445354\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[38, 2, 56, 41, 38, 47, 2, 37, 51, 54, 47, 44, 2, 56, 41, 34, 53, 2, 41, 38, 1, 0, 41, 34, 37, 2, 49, 45, 34, 47]\n",
      "Mean Loss: 4.243317\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[78, 1, 0, 1, 0, 77, 56, 38, 45, 45, 12, 2, 53, 41, 38, 2, 48, 47, 45, 58, 2, 53, 41, 42, 47, 40, 2, 42, 2, 36]\n",
      "Mean Loss: 4.243527\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[39, 38, 2, 42, 47, 2, 53, 41, 42, 52, 2, 56, 48, 51, 45, 37, 12, 2, 41, 38, 2, 56, 34, 52, 2, 39, 48, 51, 36, 38]\n",
      "Mean Loss: 4.2430344\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[54, 52, 2, 42, 47, 2, 53, 41, 38, 2, 49, 38, 51, 39, 48, 51, 46, 34, 47, 36, 38, 2, 48, 39, 2, 41, 42, 52, 2, 37]\n",
      "Mean Loss: 4.2429423\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[34, 36, 2, 34, 47, 37, 2, 51, 38, 35, 38, 36, 36, 34, 12, 2, 48, 39, 2, 41, 48, 56, 1, 0, 43, 34, 36, 48, 35, 2]\n",
      "Mean Loss: 4.2425838\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[2, 56, 34, 58, 13, 2, 41, 38, 2, 56, 38, 47, 53, 2, 34, 53, 2, 48, 47, 36, 38, 2, 53, 48, 2, 53, 41, 38, 2, 44]\n",
      "Mean Loss: 4.2427607\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[56, 52, 12, 2, 34, 2, 40, 34, 45, 45, 38, 51, 58, 12, 2, 46, 34, 51, 35, 45, 38, 37, 2, 56, 34, 45, 45, 52, 12, 2]\n",
      "Mean Loss: 4.2428236\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[38, 36, 54, 53, 48, 51, 2, 51, 38, 49, 45, 42, 38, 37, 2, 56, 42, 53, 41, 2, 39, 51, 42, 40, 42, 37, 2, 52, 38, 55]\n",
      "Mean Loss: 4.2423816\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[44, 52, 12, 2, 34, 47, 37, 2, 53, 41, 34, 53, 2, 56, 38, 2, 41, 34, 37, 2, 53, 48, 2, 51, 38, 52, 53, 48, 51, 38]\n",
      "Mean Loss: 4.242781\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[42, 53, 41, 2, 40, 51, 34, 53, 38, 39, 54, 45, 2, 53, 38, 34, 51, 52, 13, 2, 34, 47, 37, 2, 52, 48, 2, 53, 41, 38]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0af2594f4304>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_loss = list()\n",
    "for e in epoch:\n",
    "    count = 0\n",
    "    e_loss = list()\n",
    "    for i, o in loader:\n",
    "        model.zero_grad()\n",
    "        hidden = model.init_hidden(loader.batch_size)\n",
    "        output, hidden = model(i, hidden)\n",
    "        loss = criterion(output,o.reshape([48*loader.sequence_length]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count +=1\n",
    "        e_loss.append(loss.data.numpy())\n",
    "        if count > 5:\n",
    "            print('Mean Loss:', np.mean(e_loss[-30:]))\n",
    "            #sample = list(model.sample(torch.tensor([53,41,38])))\n",
    "            #print(''.join([loader.inx2char[int(c)] for c in sample]))\n",
    "            print([int(i) for i in output.topk(1)[1][:30]])\n",
    "            print([int(i) for i in o.reshape([48*loader.sequence_length])[:30]])\n",
    "            count=0\n",
    "    epoch_loss.append(np.mean(e_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print([int(i) for i in output.topk(1)[1][:30]])\n",
    "print([int(i) for i in o.reshape([48*loader.sequence_length])[:30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.argmax(output.reshape(48*32,-1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "o, h = model.predict(torch.tensor([[]]))\n",
    "o\n",
    "#nn.functional.softmax(oo.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.sample(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor([[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(input_size=loader.sequence_length, \n",
    "                output_size=len(loader.chars), \n",
    "                hidden_size=12, num_layers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "epoch = range(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = list()\n",
    "for e in epoch:\n",
    "    count = 0\n",
    "    e_loss = list()\n",
    "    for i, o in loader:\n",
    "        model.zero_grad()\n",
    "        hidden = model.init_hidden(loader.batch_size)\n",
    "        output, hidden = model(i, hidden)\n",
    "        loss = criterion(output,o.reshape([48*32]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count +=1\n",
    "        e_loss.append(loss.data.numpy())\n",
    "        if count > 30:\n",
    "            print('Mean Loss:', np.mean(e_loss[-30:]))\n",
    "            sample = list(model.sample(torch.tensor([53,41,38])))\n",
    "            print(''.join([loader.inx2char[int(c)] for c in sample]))\n",
    "            count=0\n",
    "    epoch_loss.append(np.mean(e_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(output.reshape(48*32,-1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, h = model.predict(torch.tensor([[]]))\n",
    "o\n",
    "#nn.functional.softmax(oo.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
